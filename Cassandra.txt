# Cassandra 101 Wide Store DDBB [[{db_engine.wide_column.cassandra,db_engine.101,]]

## Don't use Cassandra 

### Discord migrates from Cassandra to ScyllaDB to escalate.
* <https://www.infoq.com/news/2023/06/discord-cassandra-scylladb/>
  From 177 Cassandra nodes to 72 ScyllaDB nodes while reducing tail latencies
  for reads and writes.
* ScyllaDB (C++) is compatible with Cassandra (Java).
* C&P from: <https://www.freecodecamp.org/news/scylladb-its-cassandra-but-better-76e3d83a4f81/>
  ... for most cases, Scylla’s 99.9 percentile latency is 5–10X better than Cassandra’s.
   Also in the benchmarks mentioned here, a standard 3 node Scylla cluster offers
  almost the same performance as a 30 node Cassandra cluster (which leads to a 10X 
  reduction in cost). [[{PM.billing,cloud.price}]]
  ...  Cassandra relies on threads for parallelism. The problem is 
  that threads require a context switch, which is slow. ...
  ScyllaDB uses the seastar framework to shard requests on each core. 
  The application only has one thread per core. This way, if a session 
  is being handled by core 1 and a query request for that session comes 
  to core 2, it’s directed to core 1 for processing. Any of the cores 
  can handle the response after that. ... each thread has its own memory/cpu
  and NIC buffer queues.

### Why It's a Poor Choice For a Metadata Database for Object Stores

* REF: <https://blog.min.io/the-trouble-with-cassandra-based-object-stores/>
- Cassandra excels at supporting write-heavy workloads.
- Cassandra have limitations when supporting read-heavy workloads
  due to its eventual consistency model and lack of transactions,
  multi-table support like joins, subqueries can also limit its usefulness.
- Object storage needs are far simpler and different from what Cassandra
  is built for.
- WARN: Because the implications of employing Cassandra as a object
  storage metadata database were not properly understood, many object
  storage vendors made it a foundational part of their architecture.
  keeping them from ever moving past simple archival workloads.
- NOTE: It is not unusual to see Cassandra clusters with 100+ nodes in
  production enviroments.

## Use Cassandra for ...
* Cassandra whitepaper:
  - DISTRIBUTED KEY-VALUE (of KEY-VALUE) STORE.
  - developed at Facebook.
  - data and processing spread out across many commodity servers
  - highly available service without single point of failure
    allowing replication even across multiple data centers.
  - Possibility to choose synchronous/asynchronous replication
    for each update.
  - fully distributed DB, with no master DB.
  - Reads are linearly scalable with the number of nodes.
    It scalates (much)better cassandra that comparables systems
    like hbase,voldermort voldtdb redis mysql
    Writes are linearyly scalable?

  - based on 2 core technologies:
    - Google's Big Table
    - Amazon's Dynamo

- 2 versions of Cassandra:
  - Community Edition  : distributed under the Apache™ License
  - Enterprise Edition : distributed by Datastax

- Cassandra is a BASE system (vs an ACID system):
  BASE == (B)asically (A)vailable, (S)oft state, (E)ventually consistent)
  ACID == (A)tomicity, (C)onsistency, (I)solation, (D)urability.

  - BASE implies that the system is optimistic and accepts that
    the database consistency will be in a state of flux
  - ACID is pessimistic and it forces consistency at the end
    of every transaction.

  ```
  |                    ┌·> 1  Replication
  |                    ·      Strategy
  |                    v
  |                    1
  | cluster 1 <> 1+ Keyspace 1 <> N Column Family   <> 0+ Row
  |              └┬┘                └─────┬─────┘
  |          Typically
  |          just one               sharing similar   - collection of
  |                                 structure           sorted columns.
  | 
  |   ┌─ CQL lang. used to access data.
  | ┌─┴───────────────────────────────────────────────────────────────┐
  | │  KEYSPACE:  container for app.data, similar to a RDMS schema    │
  | │             ┌─────────────────────────────────────────────────┐ │
  | │             │ Column Family 1                                 │ │
  | │             └─────────────────────────────────────────────────┘ │
  | │             ┌─────────────────────────────────────────────────┐ │
  | │             │ Column Family 2                                 │ │
  | │             └─────────────────────────────────────────────────┘ │
  | │               ...                                               │
  | │ ┌─────────┐ ┌─────────────────────────────────────────────────┐ │
  | │ │Settings │ │ Column Family N      Col    Col2  ...  ColN     │ │
  | │ ├─────────┤ │                      Key1   Key2  ...  KeyN     │ │
  | │ │*Replica.│ │ ┌────────┐ ┌──────────────────────────────────┐ │ │
  | │ │ Strategy│ │ │Settings│ │RowKey1│Val1_1│Val2_1│   │ValN_1│ │ │ │
  | │ └─────────┘ │ │        │ │RowKey2│Val2_2│Val2_2│   │ValN_2│ │ │ │
  | │             │ └────────┘ │         ...                      │ │ │
  | │             │            └──────────────────────────────────┘ │ │
  | │             └─────────────────────────────────────────────────┘ │
  | └─────────────────────────────────────────────────────────────────┘
  |                             RowKey := Unique row UID
  |                                       Used also to sharde/balance
  |                                       load across NODES.
  |                             ValX_Y := value || value collection
  | 
  |      BASIC INFRASTRUCTURE ARCHITECTURE:
  |      ──────────────────────────────────
  |   *NODE: (Cassandra peer instance)                *RACK:
  |    - data storage (on File System                  - Set of nodes
  |      or HDFS)
  |    - The data is balanced across nodes
  |      based on the RowKey f each Column Family.
  |    - Commit log to ensure
  |      data durability.
  |      - Sequentially written                    automatically
  |                                                replicated throughout
  |            Node "N" choosen                    cluster
  |            based on RowKey                         ^
  |                v                                ┌──┴───┐
  |   Input > Commit Log > Index+write >(memTable > Write to    ··> SSTable
  |           @Node N      to memTable   full)      SSTable         Compaction
  |                           └──┬───┘              └──┬───┘        └───┬────┘
  |              - in-memory structure   - (S)orted (S)tring Table  - Periodically consolidates
  |              - "Sort-of" write-back    in File System or HDFS     storage by discarding
  |                cache                                              data marked for deletion
  |                                                                   with a tombstone.
  |                                                                 - repair mechanisms exits
  |                                                                   to ensure consistency
  |                                                                   of cluster
  | 
  |   *DATACENTER:                 *CLUSTER:
  |   - Logical Set of Racks.       - 1+ Datacenters
  |   - Ex: America, Europe,...     - full set of nodes
  |   - Replication set at            which map to a single
  |     Data Center bounds            complete token ring
  ```

## Cassandra CQL
- similar syntax to SQL
- works with table data.
- Available Shells: cqlsh | DevCenter | JDBC/ODBC drivers

## Cassandra Coordinator Node
  - Node where client connects for a read/write request
    acting as a proxy between the client and Cassandra
    internals.

## Cassandra Key Components

* Gossip: peer-to-peer communication protocol in which nodes periodically 
  (1 secs) exchange versioned state information of "well-known" nodes 
  with up to 3 "neighbours" nodes.
* Gossip information is also stored locally by each node to use immediately 
  when a node restarts.<br/> WARN: This can be dangerous if the node was
  stopped for a long time having outdated information.
* A list of seed nodes exists to bootstrap new nodes.<br/>
  In multi-data-center/multi-replication-groups clusters at least one node
  per data-center/replication-group must exist.


## Cassandra data distribution and replication

* The tuple (table, primary key) determines which node the data is stored on.
* Replicas are copies of rows. Factors influencing replication include
  Virtual nodes (Vnodes), ...

[[{doc_has.comparative]]
## Similar projects  

- ScyllaDB: C++ Real Time Big DataBase compatible with CassandraDB.
- """close-to-the-metal architecture handles millions of ops/sec with 
  predictable single-digit millisecond latencies.""".<br/>
  Looks to be a more modern alternative to Cassandra.
- Voldermort: developed by Linked-In.
  * <https://www.project-voldemort.com/voldemort/>
  * """Voldemort is not a relational database, it does not attempt to
    satisfy arbitrary relations while satisfying ACID properties.
     Nor is it an object database that attempts to transparently map 
    object reference graphs. Nor does it introduce a new abstraction 
    such as document-orientation. It is basically just a big, distributed,
    persistent, fault-tolerant hash table."""

[[doc_has.comparative}]]

# What's new:
- Cassandra 4.0 (2020-10)
<https://cassandra.apache.org/doc/latest/new/>
  - Support for Java 11
  - Virtual Tables
  - Audit Logging
    - All successful/failed login attempts
    - All database command requests to CQL.
      - high performant live query logging
      - useful for live traffic capture and traffic replay.
      - Chronicle-Queue used to rotate a log of queries
  - Full Query Logging (FQL)
  - Improved Internode Messaging
  - Improved Streaming
    - Streaming is the process used by nodes of a cluster to exchange data
      in the form of SSTables.
  - Transient Replication (experimental)

## Cassandra cstar: Orchestration Tool by Spotify
* <https://github.com/spotify/cstar>
* <www.infoq.com/news/2018/10/spotify-cstar><br/>
""" ... Cstar emerged from the necessity of running shell commands
   on all host in a Cassandra cluster ....
    Spotify fleet reached 3000 nodes. Example scripts were:
    - Clear all snapshots
    - Take a new snapshot (to allow a rollback)
    - Disable automated puppet runs
    - Stop the Cassandra process
    - Run puppet from a custom branch of our git repo
      in order to upgrade the package
    - Start the Cassandra process again
    - Update system.schema_columnfamilies to the JSON format
    - Run `nodetool upgradesstables`, which depending on
      the amount of data on the node, could take hours to
      complete
    - Remove the rollback snapshot
"""

* Install like:
  ``` $ pip3 install cstart ```

* """Why not simply use Ansible or Fabric?<br/>
   Ansible does not have the primitives required to run things in a topology aware
   fashion. One could split the C* cluster into groups that can be safely executed
   in parallel and run one group at a time. But unless the job takes almost
   exactly the same amount of time to run on every host, such a solution would run
   with a significantly lower rate of parallelism, not to mention it would be
   kludgy enough to be unpleasant to work with.<br/>
   Unfortunately, Fabric is not thread safe, so the same type of limitations apply.
   All involved machines are assumed to be some sort of UNIX-like system like
   OS X or Linux with python3, and Bourne style shell."""

## Dynamo vs Cassandra [[{doc_has.comparative]]
  https://sujithjay.com/data-systems/dynamo-cassandra/
  Dynamo vs Cassandra : Systems Design of NoSQL Databases

  State-of-the-art distributed databases represent a distillation of
  years of research in distributed systems. The concepts underlying any
  distributed system can thus be overwhelming to comprehend. This is
  truer when you are dealing with databases without the strong
  consistency guarantee. Databases without strong consistency
  guarantees come in a range of flavours; but they are bunched under a
  category called NoSQL databases.  [[}]]
[[}]]

# Cassandra TODO:

## Understanding Cassandra:
* <https://rene-ace.com/cassandra-101-understanding-what-is-cassandra/>
  - Seed node
  - Snitch purpose
  - topologies
  - Coordinator node,
  - replication factors,
  - ...

## Cassandra+Spark
<https://es.slideshare.net/chbatey/1-dundee-cassandra-101>


