# Distributed Cache [[{DOC_HAS.comparative]]

## WARN: Dont use

 When data is persistent (vs temporal) real Key-Value databases (ScyllaDB/Cassandra,...)
 are preferred. Caches are designed for short-time sessions (also using 
 simpler and faster but less reliable and scalable) key-value stores.

Use cache for reference Data that can be evicted when resources are scarse.

## Caching-First

C&P from whitepaper by Christoph Engelbert, Software architect at Hazelcast:
"""Caching-First: term to describe the situation where you start thinking
  about Caching itself as one of the main domains of your application."""


## Caching Strategies

### Cooperative (Distributed) caching:
  different  cluster-nodes work together to build a huge, shared cache
  Ussually an "intelligent" partitioning algorithm is used to balance
  load about cluster nodes.
  Common approach when system requires large amounts of data to be cached

### Partial Caching
  not all data is stored in the cache.

### Geographical Caching:
- located in chosen locations to optimize latency
- CDN (Content Delivery Network) is the best known example of this type of cache
- Works well when  content changes less often.

### Preemptive Caching
- mostly used in conjunction with a Geographical Cache
- Using a warm-up engine a Preemptive Cache is populated on startup
  and tries to update itself based on rules or events.
- The idea behind this cache addition is to reload data from any
  backend service or central cluster even before a requestor wants
  to retrieve the element. This keeps access time to the cached
  elements constant and prevents accesses to single elements from
  becoming unexpectedly long.
- Can be difficult to implement properly and requires a lot of
  knowledge of the cached domain and the update workflows

### Latency SLA Caching
- It's able to maintain latency SLAs even if the cache is slow
  or overloaded. This type of cache can be build in two different ways.
- Having a timeout to exceed before the system either requests
  the potentially cached element from the original source
  (in parallel to the already running cache request) or simple
  default answer, using whatever returns first.
- Always fire both requests in parallel and take whatever returns first.
  (discouraged since it mostly dimiss the value of caching). Can make
  sense if multiple caching layers are available.

## Caching Topologies
- In-process:
  - cache share application's memory space.
  - most oftenly used in non-distributed systems.
  - fastest possible access speed.
  - Easy to build, but complex to grow.

- Embedded Node Caches
  - the application itself will be part of the cluster.
  - kind of combination between an In-Process Cache and the
    Cooperative Caching
  - it can either use partitioning or full dataset replication.
  - CONST: Application and cache cannot be scaled independently

- Client-Server Caches
  - these systems tend to be Cooperative Caches by having a
    multi-server architecture to scale out and have the
    same feature set as the Embedded Node Caches
    but with the client layer on top.
  - This architecture keeps separate clusters of the applications
    using the cached data and the data itself, offering
    the possibility to scale the application cluster and the
    caching cluster independently.

## Evict Strategies
- Least Frequently used
  - values that are accessed the least amount of times are
    remove on memory preasure.
  - each cache record must keep track of its accesses using
    a counter which is increment only.

- Least Recently Used
  - values that were last used most far back in terms of time
    are removed on memory preasure.
  - each record keeps must track of its last access timestamp

- Other evict strategies can be found at 
  <https://en.wikipedia.org/wiki/Cache_replacement_policies>
[[}]]
