● Event Based Architecture: [[{]]
@[https://www.infoq.com/news/2017/11/jonas-reactive-summit-keynote]
Jonas Boner ... talked about event driven services (EDA) and
event stream processing (ESP)... on distributed systems.

... background on EDA evolution over time:
- Tuxedo, Terracotta and Staged Event Driven Architecture (SEDA).

ºevents represent factsº
- Events drive autonomy in the system and help to reduce risk.
- increase loose coupling, scalability, resilience, and traceability.
- ... basically inverts the control flow in the system
- ... focus on the behavior of systems as opposed to the
  structure of systems.

- TIP for developers:
 ºDo not focus on just the "things" in the systemº
 º(Domain Objects), but rather focus on what happens (Events)º
- Promise Theory:
  - proposed by Mark Burgess
  - use events to define the Bounded Context through
    the lense of promises.

quoting Greg Young:
"""Modeling events forces you to have a temporal focus on what’s going on in the
system. Time becomes a crucial factor of the system."""
""" Event Logging allows us to model time by treating event as a snapshot in time
   and event log as our full history. It also allows for time travel in the
   sense that we can replay the log for historic debugging as well as for
auditing and traceability. We can replay it on system failures and for data replication."""

Boner discussed the following patterns for event driven architecture:
- Event Loop
- Event Stream
- Event Sourcing
- CQRS for temporal decoupling
- Event Stream Processing

Event stream processing technologies like Apache Flink, Spark Streaming,
Kafka Streams, Apache Gearpump and Apache Beam can be used to implement these
design patterns.
[[}]]

[[{]]
    <span title>Summary</span><br/>
<pre zoom labels="kafka,_PM.ext_resource">
<span xsmall>External Links</span>
- Documentation:
@[http://kafka.apache.org/documentation/]

- Clients: (Java,C/C--,Python,Go,...)
@[https://cwiki.apache.org/confluence/display/KAFKA/Clients]

- Official JAVA JavaDoc API:
 @[http://kafka.apache.org/11/javadoc/overview-summary.html]

- Papers, ppts, ecosystem, system tools, ...:
@[https://cwiki.apache.org/confluence/display/KAFKA/Index]

- Kafka Improvement Proposals (KIP)
@[https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals]

- Data pipelines with Kotlin+Kafka+(async)Akka
@[../JAVA/kotlin_map.html?query=data+pipelines+kafka]

- Cheatsheets:
@[https://lzone.de/cheat-sheet/Kafka]
@[https://ronnieroller.com/kafka/cheat-sheet]
@[https://abdulhadoop.wordpress.com/2017/11/06/kafka-command-cheat-sheet/]
@[https://github.com/Landoop/kafka-cheat-sheet]
</pre>


<pre zoom labels="kafka,101,_doc_has.diagram">
<span xsmall>Global Diagram</span>
(one+ server in 1+ datacenters)

☞Bºbest practiceº: publishers must be unaware of underlying 
   LOG partitions and only specify a partition key:
   While LOG partitions are identifiable and can be sent 
   to directly, it is not recommended. Higher level 
   constructs are recommended.

┌────────┐              ┌──────────┐               ┌───────────┐
│PRODUCER│ 1 ←······→ 1 │TOPIC     │ 1 ←···→ 0...N │SUBSCRIBERS│
│        ··→ stream of··→(CATEGORY)│               │           │
└────────┘  *RECORDs*   └──────────┘               └───────────┘
  ↑          ======       1                          1 ← Normally, 1←→1, but not necesarelly
  |        Oº·key       º ↑                          ↑       
  ·        Oº·value     º ·                          ·     
  ·        Oº·timeStamp º ·                          ·
  ·                       ·                          ·  GºCONSUMER GROUPº: cluster of consumers
  ·                       ·                          ↓    |  (vs single process consumer) º*1º
(optionally) producers    ·                          1    ↓
can wait for ACK.         ↓                          CONSUMER   |    CONSUMER       ←───┐
  ·                       1                          GROUP A    |    GROUP B            │
  ·   ┌───┬────────────────────┬──────────────────┐  ---------  | --------------------   
  ·   │LOG│                    │  ordered records │  Cli1 Cli2  | Cli3 Cli4 Cli5 Cli6   │
  |   ├───┘                    │  ─────────────── │   |    |    |  |    |    |    |      
  └···→┌Partitionº0ºreplica ┌1 │  0 1 2 3 4 5     │  ←┤···········←┘    ·    ·    ·     │
      │└Partitionº0ºreplica │2┐│  0 1 2 3 4 5     │   ·    ·    |       ·    ·    ·      
      │┌Partition 1 replica ├1││  0 1 2 3         │   ·    ·    |       ·    ·    ·     │
      │└Partition 1 replica │2┤│  0 1 2 3         │  ·+···←┤    |       ·    ·    ·      
      │┌Partitionº2ºreplica ├1││  ...             │   ·    |    |       ·    ·    ·     │
      │└Partitionº2ºreplica │2┤│                  │   |···←┘    |       ·    ·    ·      
      │┌Partition 3 replica ├1││ - Partitions are │  ←┴················←┘    ·    ·     │
      │└Partition 3 replica │2┤│ independent and  │             |            ·    ·      
      │┌Partitionº4ºreplica ├1││ grow at differn. │             |            ·    ·     │
      │└Partitionº4ºreplica │2┤│ rates.           │             |            ·    ·      
      │┌Partition 5 replica ├1││                  │             |            ·    ·     │
      │└Partition 5 replica │2┤│ - Records expire │  ←······················←┘    ·      
      │┌Partitionº6ºreplica └1││ and can not be   │             |                 ·     │
      │└Partitionº6ºreplica  2┘│ deleted manually │  ←···························←┘      
      └──↑───────────────────^─┴──────────────────┘ * num. of group instances           │
                      │      │    ←── offset ─→       must be <= # partitions            
                      │      └────────────┐   record.offset (sequential id)             │
┌─────────────────────┴─────────────────┐ │   uniquelly indentifies the record           
─ Partitions serve several purposes:      │   within the partition.                     │
  ─ allow scaling beyond a single server. │                                              
  ─ act as the ☞BºUNIT OF PARALLELISMº.   │  - aGºCONSUMER GROUPºis a view (state,  ┐   │
─ Partition.ºRETENTION POLICYº indicates  │    position, or offset) of full LOG.    │
  how much time records will be available │  - consumer groups enable different     ├───┘
  for compsumption before being discarded.│    apps to have a different view of the │
-  NUMBER OF PARTITIONS IN AN EVENT HUB   │    LOG, and to read independently at    │
   DIRECTLY RELATES TO THE NUMBER OF      │    their own pace and with their own    ┘
   CONCURRENT READERS EXPECTED.           │    offsets:
-  TOTAL ORDER  OF EVENTS IS JUST GUARAN- │    * IN A STREAM PROCESSING ARCH,
   TEED INSIDE A PARTITION.               │      EACH DOWNSTREAM APPLICATION 
   Messages sent by a producer to a given │      EQUATES TO A CONSUMER GROUP
   partition are guaran. to be appended   │  
   in the order they were sent.           │  
          ┌───────────────────────────────┘           
- Messages sent by a producer to a particular topic partition are guaranteed
  to be appended in the order they are sent.
  ┌───────┴────────┐
  PARTITION REPLICA: (fault tolerance mechanism) 
  └ Kafka allows producers to wait on acknowledgement so that a write 
    isn't considered complete until it is fully replicated and guaranteed 
    to persist even if the server written to fails, allowing to balance 
    replica consistency vs performance.
  └ Each partition has one replica server acting as leader" and 0+ 
    replica servers "followers". The leader handles all read 
    and write requests for the partition while the followers passively 
    replicate the leader. If the leader fails, one of the followers 
    replaces it.
  └ A server can be a leader for partition A and a follower for 
    partition B, providing better load balancing.
  └ For a topic with replication factor N, we will tolerate up to N-1 
    server failures without losing any records committed to the log.

º*1:º
 - Log partitions are (dnyamically) divided over consumer instances so 
   that each client instance is the exclusive consumer of a "fair share"
   of partitions at any point in time.
 - The consumer group generalizes the queue and publish-subscribe:
   - As with a queue the consumer group allows you to divide (scale) up
     processing over a collection of processes (the members of the consumer group).
   - As with publish-subscribe, Kafka allows you to broadcast messages to
     multiple consumer groups
</pre>

<span title>data-schema Support</span>
<pre zoom labels="kafka.101,enterprise_patterns,_PM.TODO">
<span title>Apache AVRO format</span>
• Efficient compac way to store data in disk.
• "Imported" from Hadoop's World. 
• Efficient: Binary format, containing only data (vs JSON/JSON-schema field name+data)
  libraries (avro-tools.jar, kafka-avro-console-consumer,...) used to write/read.
• Schema never goes inside the message, only the "SchemaID". 
• Support for (fast) Google’s Snappy Compression.
• Support forºon-demand deserialization of columns (vs full data)º.
• It's not a column store (like Parquet, supporting better compression and insertion    [comparative]
  predicates), but it adapts better to big fragments of data.
@[https://www.slideshare.net/HadoopSummit/file-format-benchmark-avro-json-orc-parquet]

• Example:
  JAVA Class         ←··→ Avro Schema 
  =================       ======================
  public class User {     {
      long   id;            "type": "record",
      String name;          "name": "User",
  }                         "fields" : [
                              {"name": "id"  , "type": "long"  },
                              {"name": "name", "type": "string"}
                            ]
                          }

• No data is null by default                             [million_dolar_mistake]

• Enterprise Patterns:
  • Define global or by-product dictionary of Business events.
  • Manage Avro schemas within a Source Control Management ( "Git" )
    (Notice also Avro support for schema evolution)
    "Dump" schemas and topic relations to Confluent Schema Registry 
    when new events are validated (or existing events updated).
    event validation: Get sure they are really needed 
    (they are NOT a duplicated view of other existing event).
  • Ideally each kafka topic corresponds to a single schema.
    A (very opinionated)  "HIERARCHICAL TOPIC NAMING" can be similar to:
    ${cluster}.${tenant}.${product_or_concern}.${recipient}.${version}
                                                └────┬────┘
                                              process, person or group in charge 
                                              of reacting to event.
    This hierarchy also help in classifying different topics into 
    different "Service Levels" categories (scalability, partitions, replication 
    factor, retention time, ...) 
 
@[https://shravan-kuchkula.github.io/kafka-schemas/#understand-why-data-schemas-are-a-critical-part-of-a-real-world-stream-processing-application]
<hr/>
<span xsmall>Kafka Schema Registry</span>
BºDZone Intro Summaryº
@[https://dzone.com/articles/kafka-avro-serialization-and-the-schema-registry]
by Jean-Paul Azar

Confluent Schema Registry:
  - REST API for producers/consumers managing Avro Schemas:
    - store schemas for keys and values of Kafka records.
    - List schemas and schema-versions by subject.
    - Return schema by version or ID.
    - get the latest version of a schema.
    - Check if a given schema is compatible with a certain version. 
       - Compatibility level include:
         - backward: data written with old schema readable with new one.
         - forward : data written with new schema is readable with old one
         - full    : backward + forward
         - none    : Schema is stored by not schema validation is disabled
                   (Rºnot recommendedº).
       - configured  GLOBALLY OR PER SUBJECT.

  - Compatibility settings can be set to support EVOLUTION OF SCHEMAS.  [[qa]]

  - Kafka Avro serialization project provides serializers 
    taking schemas as input?
  - Producers send the schema (unique) ID and consumers fetch (and cache)
    the full schema from the Schema Registry.

  - Producer will create a new Avro record (schema ID, data). 
    Kafka Avro Serializer will register (and cache locally) the associated
    schema if needed, before serializing the record.

  - Schema Registry ensures that producer and consumer see compatible
    schemas and AUTOMATICALLY "TRANSFORM" BETWEEN COMPATIBLE SCHEMAS,
    TRANSFORMING PAYLOAD VIA AVRO SCHEMA EVOLUTION.
   
● SCHEMA EVOLUTION:
  Scenario:
  - Avro schema modified after data has already been written to store
    with old schema version.
 
  IMPORTANT:  ☞ From Kafka perspective, SCHEMA EVOLUTION happens only
                DURING DESERIALIZATION AT THE CONSUMER (READ):
                If consumer’s schema is different from the producer’s schema,
                and they are compatible, the value or key is automatically 
                modified during deserialization to conform to the consumer's
                read schema if possible.

  SCHEMA EVOLUTION: ALLOWED COMPATIBLE MODIFICATION
  - change/add field's default value.
  - add new         field with default value.
  - remove existing field with default value.
  - change field's order attribute.
  - add/remove      field-alias (RºWARN:º can break consumers depending on the alias).
  - change type → union-containing-original-type.

  BEST PATTERNS:
  - Provide a default value for fields in your schema.
  - Never change a field's data type.
  - Do NOT rename an existing field (use aliases instead).

  Ex: Original schema v1: 
  
  {
    "namespace": "com.cloudurable.phonebook",
    "type": "record",
    "name": "Employee",
    "doc" : "...",
    "fields": [
      {"name": "firstName", "type": "string"                            },
      {"name": "nickName" , "type": ["null", "string"] , "default" : null},
      {"name": "lastName" , "type": "string"                             },
      {"name": "age"      , "type": "int"              , "default": -1   },
      {"name": "emails"   , "type": {"type" : "array",
                                     "items": "string"}, "default":[]    },
      {"name": "phoneNum" , "type":
                                    [ "null",
                                      { "type": "record",
                                        "name": "PhoneNum",
                                        "fields": [
                                          {"name": "areaCode"   , "type": "string"},
                                          {"name": "countryCode", "type": "string", "default" : ""},
                                          {"name": "prefix"     , "type": "string"},
                                          {"name": "number"     , "type": "string"}
                                        ]
                                      }
                                    ]
      },
      {"name": "status"                                , "default" :"SALARY", 
  
                          , "type": {
                                      "type": "enum",
                                      "name": "Status",
                                      "symbols" : ["RETIRED", "SALARY",...]
                                    }                  
      }
    ]
  }

- Schema Version 2: 
  "age" field, def. value -1, added


               | KAFKA LOG |
  Producer@v2 →|Employee@v2| ··→ consumer@v.1 ·····→ NoSQL Store
               |           |     ^                    ^
               |           |     age field removed    'age' missing
               |           |     @deserialization
               |           |     
               |           |     
               |           |     consumer@ver.2 ←..... NoSQL Store
               |           |     ^                     ^
               |           |     age set to -1         'age' missing


  REGISTRY REST API USSAGE:
  └ POST New Schema
  $ curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \ 
  $   --data '{"schema": "{\"type\": …}’ \                                   
  $   http://localhost:8081/subjects/Employee/versions                       

  └ List all of the schemas:

  $ curl -X GET http://localhost:8081/subjects                               

    Using Java OkHttp client:

    package com.cloudurable.kafka.schema;
    import okhttp3.*;
    import java.io.IOException;
    public class SchemaMain {
        private final static 
        MediaType SCHEMA_CONTENT =
                MediaType.parse("application/vnd.schemaregistry.v1+json");
        private final static String EMPLOYEE_SCHEMA = "{ \"schema\": \"" ...;
        private final static String BASE_URL = "http://localhost:8081";
     
    
        private final OkHttpClient client = new OkHttpClient();

        private static newCall(Requeste request) {
            System.out.println(client.newCall(request).execute().body().string() );
        }
        private static putAndDumpBody (final String URL, RequestBody BODY) {
            newCall(new Request.Builder().put(BODY).url(URL).build());
        }
        private static postAndDumpBody(final String URL, RequestBody BODY) {
            newCall(new Request.Builder().post(BODY).url(URL).build());
        }
    
        private static getAndDumpBody(final String URL) {
            request = new Request.Builder() 
                    .url(URL).build();
            System.out.println(client.newCall(request).
                   execute().body().string());
        }
    
        public static void main(String... args) throws IOException {
            System.out.println(EMPLOYEE_SCHEMA);
    
            postAndDumpBody(                            // ← POST A NEW SCHEMA
              BASE_URL + "/subjects/Employee/versions",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            getAndDumpBody(BASE_URL + "/subjects");     // ← LIST ALL SCHEMAS
    
            getAndDumpBody(BASE_URL                     // ← SHOW ALL VERSIONS
                 + "/subjects/Employee/versions/");
    
            getAndDumpBody(BASE_URL                     // ← SHOW VERSION 2 OF EMPLOYEE
                 + "/subjects/Employee/versions/2");
    
            getAndDumpBody(BASE_URL                     // ← "SHOW SCHEMA WITH ID 3
                 + "/schemas/ids/3");
    
            getAndDumpBody(BASE_URL                     // ← SHOW LATEST VERSION
                 + "/subjects/Employee/versions/latest");
    
            postAndDumpBody(                            // ← SCHEMA IS REGISTERED?
              BASE_URL + "/subjects/Employee",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            postAndDumpBody(                            // ← //TEST COMPATIBILITY
              BASE_URL + "/compatibility/subjects/Employee/versions/latest",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            getAndDumpBody(BASE_URL                     // ← TOP LEVEL CONFIG
                 + "/config");
    
            putAndDumpBody(                            // ← SET TOP LEVEL CONFIG VALs
              BASE_URL + "/config",                    //   VALs :=none|backward|
              RequestBody.create(SCHEMA_CONTENT,                   forward|full
                 "{\"compatibility\": \"none\"}"
            );
    
            putAndDumpBody(                            // ← SET CONFIG FOR EMPLOYEE
              BASE_URL + "/config/Employee",          //
              RequestBody.create(SCHEMA_CONTENT,
                 "{\"compatibility\": \"backward\"}"
            );
        }
    }

BºRUNNING SCHEMA REGISTRYº
  $º$ CONFIG="etc/schema-registry/schema-registry.properties"º
  $º$ cat ${CONFIG}                                          º
  $ºlisteners=http://0.0.0.0:8081                            º
  $ºkafkastore.connection.url=localhost:2181                 º
  $ºkafkastore.topic=_schemas                                º
  $ºdebug=false                                              º

  $º$ .../bin/schema-registry-start ${CONFIG}                º
  
BºWriting Producers/Consumers with Avro Serializers/Sche.Regº

  └ start up the Sch.Reg. pointing to ZooKeeper(cluster).
  └ Configure gradle: 
    plugins {
      id "com.commercehub.gradle.plugin.avro" version "0.9.0"
    }     └─────────────┬──────────────────┘
    //    http://cloudurable.com/blog/avro/index.html
    //    transform Avro type → Java class
    //    Plugin supports:
    //    - Avro schema files (.avsc) ("Kafka")
    //    - Avro RPC IDL (.avdl)
    // $º$ gradle buildº ← generate java classesº

    group 'cloudurable'
    version '1.0-SNAPSHOT'
    apply plugin: 'java'
    sourceCompatibility = 1.8
    dependencies {
      testCompile 'junit:junit:4.11'
      compile 'org.apache.kafka:kafka-clients:0.10.2.0'  ← 
      compile "org.apache.avro:avro:1.8.1"               ← Avro lib
      compile 'io.confluent:kafka-avro-serializer:3.2.1' ← Avro Serializer
      compile 'com.squareup.okhttp3:okhttp:3.7.0'
    }
    repositories {
        jcenter()
        mavenCentral()
        maven { url "http://packages.confluent.io/maven/" }
    }
    avro {
        createSetters = false
        fieldVisibility = "PRIVATE"
    }
  └ Setup producer to use GºSchema Registryº and BºKafkaAvroSerializerº
    package com.cloudurable.kafka.schema;
    import com.cloudurable.phonebook.Employee;
    import com.cloudurable.phonebook.PhoneNum;
    import io.confluent.kafka.serializers.KafkaAvroSerializerConfig;
    import org.apache.kafka.clients.producer.KafkaProducer;
    import org.apache.kafka.clients.producer.Producer;
    import org.apache.kafka.clients.producer.ProducerConfig;
    import org.apache.kafka.clients.producer.ProducerRecord;
    import org.apache.kafka.common.serialization.LongSerializer;
    import io.confluent.kafka.serializers.KafkaAvroSerializer;
    import java.util.Properties;
    import java.util.stream.IntStream;
    
    public class AvroProducer {
        private static Producer˂Long, Employee˃ createProducer() {
            final String
              serClassName = LongSerializer.class.getName();
              KafkaAvroClN = Serializer    .class.getName();
              SCHEMA_REG_URL_CONFIG = KafkaAvroSerializerConfig.
                                      SCHEMA_REGISTRY_URL_CONFIG;
              VAL_SERI_CLASS_CONFIG = ProducerConfig.
                                      VALUE_SERIALIZER_CLASS_CONFIG
    
            final Properties props = new Properties();
            props.put(ProducerConfig.   BOOTSTRAP_SERVERS_CONFIG , "localhost:9092");
            props.put(ProducerConfig.           CLIENT_ID_CONFIG , "AvroProducer"  );
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG , serClassName    );
          Bºprops.put(                     VAL_SERI_CLASS_CONFIG , KafkaAvroClN    );º
          Bº//        └────────────────────────┬──────────────────────────────┘      º
          Bº//        CONFIGURE KafkaAvroSerializer.                                 º
          Gºprops.put(                     SCHEMA_REG_URL_CONFIG , º // ← Set Schema Reg.
          Gº                              "http://localhost:8081");º      URL
            return new KafkaProducer˂˃(props);
        }
    
        private final static String TOPIC = "new-employees";
    
        public static void main(String... args) {
            Producer˂Long, Employee˃ producer = createProducer();
            Employee bob = Employee.newBuilder().setAge(35)
                    .setFirstName("Bob").set...().build();
            IntStream.range(1, 100).forEach(index->{
                producer.send(new ProducerRecord˂˃(TOPIC, 1L * index, bob));
            });
            producer.flush();
            producer.close();
        }
    }
  └ Setup consumer to use GºSchema Registryº and BºKafkaAvroSerializerº
    package com.cloudurable.kafka.schema;
    import com.cloudurable.phonebook.Employee;
    import io.confluent.kafka.serializers.KafkaAvroDeserializer;
    import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;
    import org.apache.kafka.clients.consumer.Consumer;
    import org.apache.kafka.clients.consumer.ConsumerConfig;
    import org.apache.kafka.clients.consumer.ConsumerRecords;
    import org.apache.kafka.clients.consumer.KafkaConsumer;
    import org.apache.kafka.common.serialization.LongDeserializer;
    import java.util.Collections;
    import java.util.Properties;
    import java.util.stream.IntStream;
    public class AvroConsumer { 
      private final static String BOOTSTRAP_SERVERS = "localhost:9092";
      private final static String TOPIC = "new-employees";
      private static Consumer<Long, Employee> createConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "KafkaExampleAvroConsumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                  LongDeserializer.class.getName());
        //USE Kafka Avro Deserializer.
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
          KafkaAvroDeserializer.class.getName());
    
        //Use Specific Record or else you get Avro GenericRecord.
        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true");
    
        //Schema registry location.
        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG,
                "http://localhost:8081"); //<----- Run Schema Registry on 8081
        return new KafkaConsumer<>(props);
      }

      public static void main(String... args) {
          final Consumer<Long, Employee> consumer = createConsumer();
          consumer.subscribe(Collections.singletonList(TOPIC));
          IntStream.range(1, 100).forEach(index -> {
              final ConsumerRecords<Long, Employee> records =
                      consumer.poll(100);
              if (records.count() == 0) {
                  System.out.println("None found");
              } else records.forEach(record -> {
                  Employee employeeRecord = record.value();
                  System.out.printf("%s %d %d %s \n", record.topic(),
                          record.partition(), record.offset(), employeeRecord);
              });
          });
      }
    }

    Notice that just like with the producer, we have to tell the 
    consumer where to find the Registry, and we have to configure the 
    Kafka Avro Deserializer.

Configuring Schema Registry for the consumer:

//Use Kafka Avro Deserializer.

props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,

                KafkaAvroDeserializer.class.getName());  

//Use Specific Record or else you get Avro GenericRecord.

props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true"); //Schema registry location.
props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG,
                "http://localhost:8081"); //<----- Run Schema Registry on 8081

An additional step is that we have to tell it to use the generated 
version of the Employee object. If we did not, then it would use the 
Avro GenericRecord instead of our generated Employee object, which is 
a SpecificRecord. To learn more about using GenericRecord and 
generating code from Avro, read the Avro Kafka tutorial as it has 
examples of both.


https://docs.confluent.io/current/schema-registry/index.html
https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html
</pre>
[[}]]

[[{]]
<span title>APIs</span><br/>
[[{]]
<pre zoom labels="kafka,_PM.ext_resource,_doc_has.comparative">
<span xsmall>API Summary</span>
@[https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e]
┌──────────────────────────────────────────────────────────────────┐
│   ┌───────────────┐                           ┌──────────────┐   │
│   │Schema Registry│                           │Control Center│   │
│   └───────────────┘                           └──────────────┘   │
│                                                                  │
│                                                                  │
│     Kafka 2 ←·· Replicator ┐    ┌.. REST                         │
│                            ·    ·   Proxy                        │
│  Event                     ·    ·                                │
│  Source                  ┌─v────v──┐                             │
│ (IoT,log, ─→ Producer ───→         ───→ Consumer ──→ "real time" │
│  ...)                    │ Kafka 1 │                  action     │
│                          │         │                             │
│                          │         │                             │
│                          │         │                             │
│  Data                    │         │                 Target DDBB,│
│  Source   ─→ Connect  ───→         ───→  Connect ──→ S3,HDFS, SQL│
│ (DDBB,       Source      │         │     Sink        MongoDB,... │
│  csv,...)                └─^────^──┘                             │
│                            │    │                                │
│                            │   ┌───────────┐                     │
│                        Streams │KSQL Server│                     │
│                        API     └───────────┘                     │
└──────────────────────────────────────────────────────────────────┘
  APIs            Ussage Context
  ──────────────  ───────────────────────────────────────────────────────
  Producer        Apps directly injecting data into Kafka
  ──────────────  ───────────────────────────────────────────────────────
  Connect Source  Apps inject data into CSV,DDBB,... Conn.Src API inject 
                  such data into Kafka.
  ──────────────  ───────────────────────────────────────────────────────
  Streams/KSQL    Apps consuming from Kafka topics and injecting back
                  into Kafka:
                  - KSQL   : SQL declarative syntax
                  - Streams: "Complex logic" in programmatic java/...
  ──────────────  ───────────────────────────────────────────────────────
  Consumer        Apps consuming a stream,  and perform "real-time" action
                  on it (e.g. send email...)
  ──────────────  ───────────────────────────────────────────────────────
  Connect Sink    Read a stream and store it into a target store 
  ──────────────  ───────────────────────────────────────────────────────

Producer API:
└ Bºextremely simple to useº: send data and Wait in callback.
└ RºLot of custom code for ETL alike appsº:
    - How to track the source offsets? 
      (how to properly resume your producer in case of errors)
    - How to distribute load for your ETL across many producers?
    ( Kafka Connect Source API recommended in those cases)

Connect Source API:
└ High level API built on top of the Producer API for:
  -  producer tasks Bºdistribution for parallel processingº
  -Bºeasy mechanism to resume producersº
└Bº"Lot" of available connectorsº out of the box (zero-code).

Consumer API:
└ BºKISS APIº: It uses Consumer Groups. Topics can be consumed in parallel.
             RºCare must be put in offset management and commits, as wellº
             Rºas rebalances and idempotence constraints, they’re really º
             Rºeasy to write.                                            º
             BºPerfect for stateless workloadsº (notifications,...)
└ RºLot of custom code for ETL alike appsº:

Connect Sink API:
└  built on top of the consumer API.
└Bº"Lot" of available connectorsº out of the box (zero-code).

Streams API:
└ Support for Java and Scala.
└ It enables to write either:
  -BºHigh Level DSLº(ApacheºSpark alikeº)
  -  Low Level API  (ApacheºStorm alikeº).
└ Complicated Coding is still required, but producers/consumers handling
  completely is hidden.
Bºfocussing on stream logicº
└BºSupport for 'joins', 'aggregations', 'exactly-once' semmantics.º
└Rºunit test can be difficultº (test-utils library to the rescue).
└ Use of state stores, when backed up by Kafka topics, will force
Rºprocessing a lot more messagesº, butBºadding support for resilient appsº.


KSQL :
└ ºwrapper on top of Kafka Streamsº.
└  It abstract away Stream coding complexity.
└RºNot support for complex transformations, "exploding" arrays,...º
   (as of 2019-11, gaps can be filled)

<hr/>
<pre zoom labels="kafka,_PM.TODO" bgorange>
<span xsmall>Message Delivery Semantics</span>
@[http://kafka.apache.org/documentation.html#semantics]
</pre>

<pre zoom labels="kafka,_doc_has.code_snippet">
<span xsmall>producer</span>
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/clients/producer/kafkaproducer.html]
- send streams of data to topics in the cluster.
- thread safe → sharing a singleton across threads will generally be faster. 

- ex:
     import org.apache.kafka.clients.producer.KafkaProducer;
     // pre-setup:
     final properties kafkaconf = new Properties();
     kafkaconf.put("bootstrap.servers", "localhost:9092");
     kafkaconf.put("acks"             , "all");  ← "all" : slowest/safest setting: block until
                                                   full commit of the record (in all partitions?)

     // set how to turn key|value ProducerRecord instances into bytes.
     kafkaconf.put("key.serializer"  , "org.apache.kafka.common.serialization.StringSerializer");
     kafkaconf.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
     producer˂string, string˃ producer = new kafkaproducer˂˃(kafkaconf);

     producer.send(                        // ← sending-topics is async: (added to buffer 
         new ProducerRecord˂string, string˃(    of pending records)
             "my-topic",
             integer.tostring(i),
             integer.tostring(i)
         )
     );
     producer.close(); // ← leak of resources if not done
     
    producer: 
    · pool of buffer space holding records not yet transmitted for each partition
      of size (config) batch.size

  + · background i/o thread turning
      "record"  to → (batch) network request.
       ------------------------
        automatically retry unless 
        config.retries == 0
      RºWARNº: config.retries ˃ 0 opens up the possibility of duplicates .
               Kafka0.11+ idempotent mode avoid the risk
               -"enable.idempotence" setting - 
             RºWARNº: avoid application level re-sends in this case.
             See more details in official doc.

    tunning º"linger.ms"º tells producer to wait up to that number of milliseconds
    before sending a request in hope that more records will arrive to fill up the
    same batch. This can increase performance by introducing a determenistic delay
    of at-least "linger.ms".
  Bº(similar to nagle's algorithm in tcp)º.


  - Kafka 0.11+ transactional producer allows clients to send messages to multiple
    partitions and topics! atomically (producer.beginTransaction, producer.send, 
    producer.commit, producer.abortTransaction ).
<hr/>
<span xsmall>config</span>
see full list @ @[http://kafka.apache.org/documentation/#producerconfigs]
-ºcleanup.policyº      : "delete"* or "compact" retention policy for old log segments. 
-ºcompression.typeº    : 'gzip', 'snappy', lz4, 'uncompressed' 
-ºindex.interval.bytesº: how frequently kafka adds an index entry to it's offset index.
-ºleader.replication.throttled.replicasº:  ex:
                         [partitionid]º:[brokerid],[partitionid]:[brokerid]:... or
                         wildcard '*' can be used to throttle all replicas for this topic.                            
-ºmax.message.bytesº   : largest record batch size allowed
-ºmessage.format.versionº: valid apiversion ( 0.8.2, 0.9.0.0, 0.10.0, ...)
-ºmessage.timestamp º  : max dif allowed between the timestamp when broker receives
 º.difference.max.msº    a message and timestamp specified in message.
                         
-ºmessage.timestamp.typeº: "createtime"|"logappendtime"
-ºmin.insync.replicasº : all|-1|"n": number of replicas that must acknowledge a write
                         for it to be considered  successful.
-ºretention.msº        : time we will retain a log before old log segments are discarded.
                       bºsla on how soon consumers must read their dataº.
</pre>


<pre zoom labels="api,code_snippet">
<span xsmall>Consumer</span>
@[http://kafka.apache.org/26/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html]
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html]

Class KafkaConsumer˂K,V˃

- client consuming records from cluster.
- transparently handles failure of Kafka brokers,
- transparently adapts as topic partitions it fetches migrate within the cluster.
- This client also interacts with the broker to 
 ºallow consumer-groups to load balance consumption º.
- consumer Rºis not thread-safeº.
- Compatible with brokers ver. 0.10.0+.

Basics:
- Consumer  Position: offset of the next record that will be given out.
- Committed position: last offset stored securely. Should the process
                      fail and restart, this is the offset consumers will
                      recover to.
  - Consumer can either automatically commit offsets periodically; or
    commit it manually.(commitSync, commitAsync).

- Consumer Groups and Topic Subscriptions:
  - consumer groups: consumer instances sharing same group.id creating a 
                     pool of processes (same or remote machines) to split
                     the processing of records.
  - Each consumer in a group can dynamically set the list of topics it 
    wants to subscribe to through one of the subscribe APIs. 
  - Kafka will deliver each message in the subscribed topics to one process
    in each consumer group by balancing partitions between all members so that
  Bºeach partition is assigned to exactly one consumer in the groupº. 
    (It makes no sense to have more consumers that partititions)
    Group rebalancing (map from partitions to consumer in group) occurs when:
    - Consumer is added/removed/not-available*1 to pool.
      *1 liveness detection time defined in "max.poll.interval.ms".
    - partition is added/removed from cluster.
    - new topic matching a subscribed regex is created.
    (Consumer groups just allow to have independent parallel multiprocess
     clients acting as a same app with no need to manual synchronization).
     (additional consumers are actually quite cheap).

    (See official doc for advanced topics on balancing consumer groups)


  - Simple Example Consumer: let Kafka dynamically assign a fair share of
                             the partitions for subscribed-to topics
    (See original doc for manual choosing partitions to consume from)
    Properties configProps = new Properties();
    configProps.setProperty("bootstrap.servers", "localhost:9092");
    configProps.setProperty("group.id", "test");
    configProps.setProperty("enable.auto.commit", "true");
    configProps.setProperty("auto.commit.interval.ms", "1000");
    configProps.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    KafkaConsumer˂String, String˃ consumer = new KafkaConsumer˂˃(configProps);
    consumer.subscribe(Arrays.asList("foo", "bar"));
    while (true) {
        ConsumerRecords˂String, String˃ records = consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord˂String, String˃ record : records)
            System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
           if (isManualCommit /*enable.auto.commit == false*/) {
               ... ;  consumer.commitSync();  ...
           }
    

    Use consumer.seek(TopicPartition, long) to skip up to some record.
    Use seekToBeginning(Collection)/seekToEnd(Collection) to go to start/end of partition. 


    See original doc for how to read Transactional Messages.
    See original doc for Multi-threaded Processing.
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#consumerconfigs]
@[http://kafka.apache.org/documentation/#newconsumerconfigs]
@[http://kafka.apache.org/documentation/#oldconsumerconfigs]
</pre>
[[}]]
[[{]]
<pre zoom labels="kafka,_PM.TODO">
<span xsmall>Connect</span>
@[http://kafka.apache.org/11/javadoc/index.html?overview-summary.html]
(<a TODO href="http://kafka.apache.org/documentation.html#connect">more info</a>)
- allows reusable producers or consumers that connect Kafka
  topics to existing applications or data systems.
<hr/>
<span xsmall>Connectors List</span>
@[https://docs.confluent.io/current/connect/connectors.html]
@[https://docs.confluent.io/current/connect/managing/connectors.html]
- Kafka connectors@github
  @[https://github.com/search?q=kafka+connector]
<hr/>
-BºHTTP Sinkº
-BºFileStreamºs (Development and Testing)
-BºGitHub Sourceº
-BºJDBC (Source and Sink)º
-  PostgresSQL Source (Debezium)
-  SQL Server Source (Debezium)
-BºSyslog Sourceº
- AWS|Azure|GCD|Salesforce "*"
- ...
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#connectconfigs]
</pre>

<pre zoom labels="kafka,_PM.TODO">
<span xsmall>AdminClient</span>
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/clients/admin/AdminClient.html]

- administrative client for Kafka, which supports managing and inspecting
  topics, brokers, configurations and ACLs.
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#adminclientconfigs]
</pre>

<pre zoom labels="kafka,architecture.event_stream,_PM.TODO">
<span xsmall>Streams</span>
@[http://kafka.apache.org/25/documentation/streams/]
See also:
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html]
- Built on top o producer/consumer API.

- simple lightweight embedableBºclient libraryº, with support for
  real-time querying of app state with low level Processor API
  primitives plus high-level DSL.

- transparent load balancing of multiple instances of an application.
  using Kafka partitioning model to horizontally scale processing
  while maintaining strong ordering guarantees.

- Supports fault-tolerant local state, which enables very fast and 
 efficient stateful operations likeºwindowed joins and aggregationsº.

- Supports exactly-once processing semantics when there is a 
  client|Kafka failure.

- Employs one-record-at-a-time processing to achieve millisecond 
  processing latency, and supports event-time based windowing 
  operations with out-of-order arrival of records.

  import org.apache.kafka.common.serialization.Serdes;
  import org.apache.kafka.common.utils.Bytes;
  import org.apache.kafka.streams.KafkaStreams;
  import org.apache.kafka.streams.StreamsBuilder;
  import org.apache.kafka.streams.StreamsConfig;
  import org.apache.kafka.streams.kstream.KStream;
  import org.apache.kafka.streams.kstream.KTable;
  import org.apache.kafka.streams.kstream.Materialized;
  import org.apache.kafka.streams.kstream.Produced;
  import org.apache.kafka.streams.state.KeyValueStore;
   
  import java.util.Arrays;
  import java.util.Properties;
   
  public class WordCountApplication {
   
    public static void main(final String[] args) throws Exception {
      final Properties props = new Properties();
      props.put(StreamsConfig.APPLICATION_ID_CONFIG    ,
                "wordcount-app");
      props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG ,
                "kafka-broker1:9092");
      props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, 
                Serdes.String().getClass());
      props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
                Serdes.String().getClass());
  
      final StreamsBuilder builder = new StreamsBuilder();
      KStream˂String, String˃ txtLineStream 
        = builder.stream("TextLinesTopic");
      KTable˂String, Long˃ wordCounts = txtLineStream
          .flatMapValues(
                          textLine -˃
                          Arrays.asList(
                             textLine.toLowerCase().split("\\W+")
                          )
                        )
          .groupBy      ( (key, word) -˃ word )
          .count        ( Materialized.
                          ˂  String, Long, KeyValueStore˂Bytes, byte[]˃ ˃
                          as("counts-store"));
      wordCounts.
        toStream().
        to("WordsWithCountsTopic", 
           Produced.with(Serdes.String(), Serdes.Long()));
  
      KafkaStreams streams = new KafkaStreams(builder.build(), props);
      streams.start();
    }
  }

BºKafka Streams CORE CONCEPTSº
@[http://kafka.apache.org/25/documentation/streams/core-concepts]

  └ºStreamº   : graph of stream processors (nodes) that
   ºProcessorº  connected by streams (edges).
   ºtopologyº   

  └ºStreamº   : It represents an data set unbounded in size / time, 
                ordered, replayable and fault-tolerant inmmutable 
                record set.
 
  └ºStreamº   : node processing an step to transform data
   ºprocessorº  in streams.
                - It receives one input record at a time from
                  upstream processors and produces 1+ output records.

                - Special processors:
                  - Source Processor: NO upstream processors,
                    just one or multiple Kafka topics as input.

                  - Sink Processor: no down-stream processors. 
                    Outpus goes to Kafka topic/external system.

  └two ways to define the stream processing topology:
   - Streams DSL  : map, filter, join and aggregations
   - Processor API: (low-level) Custom processors. It also allows
                    to interact with state stores.

  └ºTime modelº: operations like windowing are defined based 
                 on time boundaries.  Common notions of time in
                streams are:
    - Event      time: 
    - Ingestion  time: time it's stored into a topic partition.
    - Processing time: It may be (milli)seconds for real time or
                       hours for batch time, after event time.
                       real event time.

    - Kafka 0.10.x+ automatically embeds (event or ingestion) time.
      Event of ingestion choose can be done at Kafka or topic level 
      in configuration.
    - Kafka Streams assigns a TS to every data record via the 
      TimestampExtractor interface allowing to describe the "progress"
      of a stream with regards to time and are leveraged by 
      time-dependent operations such as window operations. 

    - time only "advances" when a new record arrives at the 
      processor.  Concrete implementations of the TimestampExtractor
      interface will provide different semantics to the
      stream time definition.

    - Finally, Kafka Streams sinks will also assign timestamps
      in a way that depends on the context:
      - When output is generated from some input record,
        for example, context.forward(), TS is  inherited from input.
      - When new output record is generated via periodic
        functions such as Punctuator#punctuate(), TS is defined
        as current node internal time (context.timestamp()) .
      - For aggregations, result update record TS is the max.
        TS of all input records.
      NOTE: default behavior can be changed in the Processor API
            by assigning timestamps to output records explicitly 
            in "#forward()".

 
  └ºAggregationº: 
   ºOperation  º  
          INPUT                      OUTPUT
          --------                   ------
          KStream   → Aggregation →  KTable
       or KTable         
          ^             ^              ^
          DSL         Ex: count/sum  DSL object:
                                     - new value is considered
                                       to overwrite the old value
                                      ºwith the same keyºin next
                                       steps.

  └ºWindowingº:  - trackedºper record keyº.
                 - Available in ºStreams DSLº. 
                 - window.ºgrace_periodº controls 
                  ºhow long Streams clients will wait forº
                  ºout-of-order data records.º
                 - Records arriving "later" are discarded.
                   "late" == record.timestamp dictates it belongs 
                             to a window, but current stream time
                             is greater than the end of the window 
                             plus the grace period.

  └ºStates   º:  - Needed by some streams. 
                 -Bºstate storesº in Stream APIs allows apps to
                   store and query data, needed by stateful operations.
                 - Every task in Kafka Streams embeds 1+ state stores
                   that can be accessed via APIs to store and query
                   data required for processing. They can be:
                   -ºpersistent key-value storeº:
                   -ºIn-memory hashmapº
                   - "another convenient data structure".
                 - Kafka Streams offers fault-tolerance and automatic
                   recovery for local state-stores.
                 - directºread-only queriesºof the state stores
                   is provided to methods, threads, processes or
                   applications external to the stream processing
                   app through BºInteractive Queriesº, exposing the
                   underlying implementation of state-store read 
                   methods.

  └ºProcessingº: - at-least-once delivery 
   ºGuaranteesº    (processing.guarantee=exactly_once  in config)
                 - exactly-once processing semantics (Kafka 0.11+)
                                                      ^^^^^^^^^^
                Kafka 0.11.0+ allows producers to send messages to
                different topic partitions in transactional and
                idempotent manner.
                More specifically,Streams client APIguarantees that 
                for any record read from the source Kafka topics,
                its processing results will be reflected exactly once
                in the output Kafka topic as well as in the 
                state stores for stateful operations.
                (KIP-129 lecture recomended)


  └ºOut-of-Orderº:
   ºHandlingº
      - Within topic-partition:
        - records with larger timestamps but smaller offsets
          are processed earlier.
      - Within stream task processing "N" topic-partitions:
        - If app is Rºnot configured to wait for all partitionsº
        Rºto contain some buffered dataº and pick from the
          partition with the smallest timestamp to process
          the next record, timestamps may be smaller in 
          following records for different partitions.
          "FIX": Allows applications to wait for longer time
                  while bookkeeping their states during the wait time.
                  i.e. making trade-off decisions between latency,
                 cost, and correctness.
                 In particular, increase windows grace time.
           Rº As for Joins some "out-of-order" data cannot be handled
              by increasing on latency and cost in Streams yet:

<hr/>
<span xsmall>Config</span>
See details: @[http://kafka.apache.org/documentation/#streamsconfigs]
-ºCore config:º
  -ºapplication.id    º: string unique within the Kafka cluster
  -ºbootstrap.servers º: host1:port1,host2:port2
                         ^^^^^^^^^^^^^^^^^^^^^^^
                         No need to add all host. Just a few ones to start sync
  -ºreplication.factorº: int, Default:1
  -ºstate.dir         º: string, Default: /tmp/kafka-streams
-ºOther params:º
  - cache.max.bytes.buffering: long, def: 10485760 (max bytes for buffering ºacross all threadsº)
  - client.id          : ID prefix string used for the client IDs of internal consumer,
                         producer and restore-consumer, with pattern '-StreamThread--'.
                         Default: ""
  - default.deserialization.exception.handler  
  - default.key  .serde                   : Default serializer / deserializer class 
  - default.value.serde
  - default.production.exception.handler: Exception handling class 
  - default.timestamp.extractor
  - max.task.idle.ms : long, Maximum amount of time a stream task will stay idle 
                       when not all of its partition buffers contain records, 
                       to avoid potential out-of-order record processing 
                       across multiple input streams. 
  - num.standby.replicas: int (default to 0)
  - num.stream.threads : 
  - processing.guarantee: at_least_once (default) | exactly_once.
                                                    ^^^^^^^^^^^^
                         - It requires 3+ brokers  in production
                         - for development it can be changed by
                           tunning broker setting 
                           - transaction.state.log.replication.factor
                           - transaction.state.log.min.isr.
  - security.protocol   : PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
  - topology.optimization: [none*, all] Set wheher Kafka Streams should optimize the topology
  - application.server  : Default: "", endpoint used for state store discovery and
                          interactive queries on this KafkaStreams instance.
  - buffered.records.per.partition: int, default to 1000 
  - built.in.metrics.version
  - commit.interval.ms   : frequency to save the position of the processor.
                          (default to 100 for exactly_once or 30000 otherwise).
  - connections.max.idle.ms: Default:    540000
  - metadata.max.age.ms: 
  - metric.reporters : 
  - metrics.num.samples: 
  - metrics.recording.level: [INFO, DEBUG]
  - metrics.sample.window.ms
  - poll.ms  : Amount of time in milliseconds to block waiting for input.
  - receive.buffer.bytes: int, def: 32768, size of the TCP receive buffer (SO_RCVBUF) to use
                          when reading  data. Set to -1 to use OS default.
  - send.buffer.bytes   : in, def: 131072, size of the TCP send   buffer (SO_SNDBUF ) to use
                           when sending data. Set to -1 to use OS default.
  - reconnect.backoff.max.ms: 
  - reconnect.backoff.ms
  - request.timeout.ms
  - retries             : Setting a value greater than zero will cause the client to 
                          resend any request that fails with a potentially transient error.
  - retry.backoff.ms    :
  - rocksdb.config.setter:
  - state.cleanup.delay.ms:
  - upgrade.from : 
  - windowstore.changelog.additional.retention.ms:
<hr/>
<span xsmall>Examples</span>
@[https://github.com/confluentinc/kafka-streams-examples]
  - Examples: Runnable Applications
  - Examples: Unit Tests
  - Examples: Integration Tests
  - Docker Example: Kafka Music demo application
    Example ºdocker-compose.ymlº:
             └───────┬────────┘
             Services launched:
             zookeeper:
             kafka:
             schema-registry:
             kafka-create-topics:
                 ...  kafka-topics --create --topic play-events ...
                 ...  kafka-topics --create --topic song-feed   ... 
      
             kafka-music-data-generator:  ← producer
             kafka-music-application:     ← consumer
    
  - Examples: Event Streaming Platform
</pre>

<pre zoom labels="kafka,_PM.TODO">
<span bgorange xsmall>KSQL(ksqlDB)</span>
@[https://www.confluent.io/]
@[https://www.confluent.io/blog/ksql-open-source-streaming-sql-for-apache-kafka/]

See also:
- Pull Queries and Connector Management Added to ksqlDB (KSQL) Event Streaming Database for Kafka (2019-12)
@[https://www.infoq.com/news/2019/12/ksql-ksqldb-streaming-database/]
  - pull queries to allow for data to be read at a specific point in 
    time using a SQL syntax, and Connector management that enables direct 
    control and execution of connectors built to work with Kafka Connect. 
  - Until now KSQL has only been able to query continuous streams 
    ("push queries"). Now it can also read current state of a materialized 
    view using pull queries:
    These new queries can run with predictable low latency since the
    materialized views are updated incrementally as soon as new
    messages arrive.
  - With the new connector management and its built-in support for a 
    range of connectors, it’s now possible to directly control and 
    execute these connectors with ksqlDB, instead of creating separate 
    solutions using Kafka, Connect, and KSQL to connect to external data 
    sources. 
    The motive for this feature is that the development team 
    believes building applications around event streams Rºwas too complexº. 
  BºInstead, they want to achieve the same simplicity as when buildingº
  Bºapplications using relational databases.                          º

  - Internally, ksqlDB architecture is based on a distributed commit 
    log used to synchronize the data across nodes. To manage state, 
    RocksDB is used to provide a local queryable storage on disk. A 
    commit log is used to update the state in sequences and to provide 
    failover across instances for high availability.
</pre>

<pre zoom labels="enterprise_patterns,kafka,TODO">
<span xsmall>Zeebe Streams</span>
@[https://www.infoq.com/news/2019/05/kafka-zeebe-streams-workflows]
• Kafka events are sometimes part of a business process with tasks 
  spread over several microservices. To handle complex business 
  processes a workflow engine can be used, but to match Kafka it must 
  meet the same scalability Kafka provides. 
• Zeebe is a workflow engine currently developed and designed to 
meet these scalability requirements. 
</pre>


<span title>Developer Tools</span>
<pre zoom labels="kafka,_PM.low_code,protocol.client_server,_PM.TODO">
<span title>Pixy</span>
@[https://github.com/mailgun/kafka-pixy]
  Kafka-Pixy is a dual API (gRPC and REST) proxy for Kafka with 
  automatic consumer group control. It is designed to hide the 
  complexity of the Kafka client protocol and provide a stupid simple 
  API that is trivial to implement in any language.
</pre>
<pre zoom labels="kafka,_PM.low_code,computing.containerization,">
<span XSMALl>Docker Img For Developers</span>
@[https://github.com/lensesio/fast-data-dev]
- Apache Kafka docker image for developers; with Lenses (lensesio/box) 
  or Lenses.io's open source UI tools (lensesio/fast-data-dev). Have a 
  full fledged Kafka installation up and running in seconds and top it 
  off with a modern streaming platform (only for kafka-lenses-dev), 
  intuitive UIs and extra goodies. Also includes Kafka Connect, Schema 
  Registry, Lenses.io's Stream Reactor 25+ Connectors and more.

  Ex SystemD integration file:
  /etc/systemd/system/kafkaDev.service
  | #!/bin/bash
  | 
  | # Visit http://localhost:3030 to get into the fast-data-dev environment
  | 
  | [Unit]
  | Description=Kafka Lensesio
  | After=docker.service
  | Wants=network-online.target docker.socket
  | Requires=docker.socket
  | 
  | [Service]
  | Restart=always
  | # Create container if it doesn't exists with container inspect
  | ExecStartPre=/bin/bash -c "/usr/bin/docker container inspect lensesio/fast-data-dev 2> /dev/null || /usr/bin/docker run -d --name kafkaDev --net=host -v /var/backups/DOCKER_VOLUMES_HOME/kafkaDev:/data lensesio/fast-data-dev" 
  | ExecStart=/usr/bin/docker start -a    kafkaDev 
  | ExecStop=/usr/bin/docker   stop -t 20 kafkaDev 
  | 
  | [Install]
  | WantedBy=multi-user.target
</pre>

<span title>DevOps</span>
<pre zoom labels="kafka.devops,_PM.TODO">
<span xsmall>Quick start</span>
@[http://kafka.apache.org/documentation.html#quickstart]
BºPRE-SETUPº
  Download tarball from mirror @[https://kafka.apache.org/downloads],
  then untar like:
  $º$ tar -xzf - kafka_2.13-2.5.0.tgz ; cd kafka_2.13-2.5.0º

  - Start the server
  $º$ ~ bin/zookeeper-server-start.sh \                    º ← Start Zookeeper
  $º  config/zookeeper.properties 1˃zookeeper.log 2˃⅋1 ⅋   º

  $º$ ~ bin/kafka-server-start.sh  \                       º ← Start Kafka Server
  $º  config/server.properties 1˃kafka.log  2˃⅋1 ⅋ \       º

  $º$ bin/kafka-topics.sh --create --zookeeper \           º ← Create BºTESTº topic
  $º  localhost:2181  --replication-factor 1 \             º (Alt.brokers can be
  $º  --partitions 1 --topic TEST                          º  configured to  auto-create 
                              └──┘                            them when publishing to
                                                              non-existent ones)

  $º$ bin/kafka-topics.sh --list --zookeeper localhost:2181º ← Check topic
  $º$ TEST                                                 º ← Expected output           
      └──┘

  $º$ bin/kafka-console-producer.sh --broker-list          º ← Send some messages
  $º  localhost:9092 --topic TEST                          º
  $ºThis is a message                                      º
  $ºThis is another message                                º
  $ºCtrl+V                                                 º

  $º$ bin/kafka-console-consumer.sh --bootstrap-server \   º ← Start a consumer
  $ºlocalhost:9092 --topic Bºtestº --from-beginning        º
  $ºThis is a message                                      º
  $ºThis is another message                                º


  ********************************
  * CREATING A 3 BROKERS CLUSTER *
  ********************************
  $º$ cp config/server.properties config/server-1.propertiesº ← Add 2 new broker configs.
  $º$ cp config/server.properties config/server-2.propertiesº
                                  └───────────┬────────────┘ 
               ┌──────────────────────────────┤
   ┌───────────┴────────────┐      ┌──────────┴─────────────┐  
  ºconfig/server-1.properties:º   ºconfig/server-2.properties:º
   broker.id=1                     broker.id=2                 ← unique id 
   listeners=PLAINTEXT://:9093     listeners=PLAINTEXT://:9094
   log.dir=/tmp/kafka-logs-1       log.dir=/tmp/kafka-logs-2   ← avoid overwrite


  $º$ bin/kafka-server-start.sh config/server-1.properties ...º ← Start 2nd cluser
  $º$ bin/kafka-server-start.sh config/server-2.properties ...º ← Start 3rd cluser

  $º$ bin/kafka-topics.sh --create --zookeeper localhost:2181\º ← Create new topic with
  $º  --replication-factor 3 --partitions 1                   º Bºreplication factor of 3º
  $º  --topic  topic02                                        º

  $º$ bin/kafka-topics.sh --describe \                        º ← Check know which broker
  $º  --zookeeper localhost:2181 --topic topic02              º   is doing what
   (output will be similar to)
   Topic: topic02  PartitionCount:1  ReplicationFactor:3 Configs:         ← summary 
       Topic: topic02 Partition:0 Leader:1  Replicas: 1,2,0 Isr: 1,2,0 ← Partition 0
                                                            └┬┘
                                            set of "in-sync" replicas. 
                                           (subset of "replicas" currently
                                            alive and in sync with "leader")


  *****************************************
  * Using Kafka Connect to import/export  *
  * data using simple connectors          *
  *****************************************

BºPRE-SETUPº
  $º$ echo -e "foo\nbar" &gt; test.txt       º ← Prepare (input)test data

  - SETUP source/sink Connectors:
    config/connect-file-srcs.properties ← unique_connector_id, connector class,
    config/connect-file-sink.properties   ...

  $º$ bin/connect-standalone.sh \            º ← Start Bºtwo connectorsº running in 
  $º  \                                      º   standalone mode (dedicated process)
  $º  config/connect-standalone.properties \ º ← 1st param is common Kafka-Connect config 
  $º  \                                      º   (brokers, serialization format ,...)
  $º  config/connect-file-srcs.properties  \ º 
  $º  config/connect-file-sink.properties    º 
    └─────────────────┬────────────────────┘
    examples in kafka distribution set the "pipeline" like:
    "test.txt" → connect-test  → sink connector → "test.sink.txt"


• See also: Ansible Install:
@[https://github.com/confluentinc/cp-ansible]
@[https://docs.confluent.io/current/installation/cp-ansible/index.html]
  - Installs Confluent Platform packages.
    - ZooKeeper
    - Kafka
    - Schema Registry
    - REST Proxy
    - Confluent Control Center
    - Kafka Connect (distributed mode)
    - KSQL Server
  
  - systemd Integration
  
  - Configuration options for:
     plaintext, SSL, SASL_SSL, and Kerberos.
<br/>
<span title>Configuration</span>
• Broker Config:
@[http://kafka.apache.org/documentation/#configuration]
  Main params:
  - broker.id
  - log.dirs
  - zookeeper.connect

• Topics config: @[http://kafka.apache.org/documentation/#topicconfigs"

• Compaction: kafka.devops, kakfa.performance
http://kafka.apache.org/documentation.html#compaction
</pre>
<pre zoom labels="kafka.devops.k8s,_PM.low_code,kafka.security,_PM.TODO">
<span title>Strimzi k8s Operator</span>
  "Kafka on K8s in a few minutes"
• Strimzi: RedHat OOSS project that provides container images and 
  operators for running production-ready Kafka on k8s and OpenShift. 
• K8s native experience, using kubectl to manage the Kafka cluster and GitOps.
• Monitoring and observability integration with Prometheus.
• TODO:
@[https://developers.redhat.com/blog/2019/06/06/accessing-apache-kafka-in-strimzi-part-1-introduction/]
@[https://developers.redhat.com/blog/2019/06/07/accessing-apache-kafka-in-strimzi-part-2-node-ports/]
@[https://developers.redhat.com/blog/2019/06/10/accessing-apache-kafka-in-strimzi-part-3-red-hat-openshift-routes/]
@[https://developers.redhat.com/blog/2019/06/11/accessing-apache-kafka-in-strimzi-part-4-load-balancers/]
@[https://developers.redhat.com/blog/2019/06/12/accessing-apache-kafka-in-strimzi-part-5-ingress/]

 
</pre>
<span title>Kafka vs ...</span>
<pre zoom labels="architecture.event_stream.pulsar,kafka,_doc_has.comparative,_PM.TODO">
<span title>Pulsar vs Kafka</span>
- Pulsar is a younger project Bºinspired and informed by Kafkaº. 
  Kafka on the other side has a bigger community. 

- Pulsar Webinars                                          [resource]
@[https://streamnative.io/resource#pulsar]


BºPulsas "PRO"s over Kafkaº
Bº========================º
@[https://kafkaesque.io/7-reasons-we-choose-apache-pulsar-over-apache-kafka/]
@[https://kesque.com/5-more-reasons-to-choose-apache-pulsar-over-kafka/]

1. Streaming and queuing Come together:
   Pulsar supports standard message queuing patterns, such as
   competing consumers, fail-over subscriptions, and easy message 
   fan out keeping track of the client read position in the topic 
   and stores that information in its high-performance distributed 
   ledger, Apache BookKeeper, handling many of the use cases of a 
   traditional queuing system, like RabbitMQ.
 
2. Simpler ussage:
   If you don't need partition you don't have to worry about them.
   "If you just need a topic, then use a topic". Do not worry about
   how many consumers the topic might have. 
   Pulsar subscriptions allow you to add as many consumers as you want 
   on a topic with Pulsar keeping track of it all. If your consuming 
   application can’t keep up, you just use a shared subscription to 
   distribute the load between multiple consumers.
   Pulsar hasºpartitioned topicsºif you need them, but 
  ºonly if you need themº.

3. Fitting a log on a single server becomes a challenge (Disk full,
   remote copy o large logs can take a long time.
   More info at "Adding a New Broker Results in Terrible Performance"
 @[https://www.confluent.io/blog/stories-front-lessons-learned-supporting-apache-kafka/]
   Apache Pulsar breaks logs into segments and distributes them across
   multiple servers while the data is being written by using BookKeeper
   as its storage layer. 
   This means that the Bºlog is never stored on a single serverº, so a
   single server is never a bottleneck. Failure scenarios are easier to
   deal with and Bºscaling out is a snap: Just add another server.º
 BºNo rebalancing needed.º

4. Stateless Brokers:
   In Kafka each broker contains the complete log for each of
   its partitions. If load gets too high, you can't simply add
   another broker. Brokers must synchronize state from other 
   brokers that contain replicas of its partitions.
   In Pulsar brokers accept data from producers and send data
   to consumers, but the data is stored in Apache BookKeeper.
   If load gets high, just add another broker.
   It starts up quickly and gets to work right away.

5. Geo-replication is a first-class feature in Pulsar.
   (vs proprietary add-on).
 BºConfiguring it is easy and it just works. No PhD needed.º
   
6. Consistently Faster
 BºPulsar delivers higher throughput along with lower and moreº
 Bºconsistent latency.º

7. All Apache Open Source
   input and output connectors (Pulsar IO), SQL-based topic queries 
   (Pulsar SQL), schema registry,...
   (vs Kafka open-source features controlled by a commercial entity)

8. Pulsar can have multiple tenants and those tenants can have 
   multiple namespaces to keep things all organized. Add to that
   access controls, quotas, and rate-limiting for each namespace 
   and you can imagine a future where we can all get along using
   just this one cluster.
   (WiP or Kafka in KIP-37).

9. Replication
   You want to make sure your messages never get lost. In Kakfa
   you configure 2 or 3 replicas of each message in case 
   something goes wrong.
   In Kafka the leader stores the message and the followers make
   a copy of it. Once enough followers acknowledge they’ve got it,
   "Kafka is happy".
   Pulsar uses a quorum model: It sends the message out to a 
   bunch of nodes, and once enough of them acknowledge they've
   got it, "Pulsar is happy". Majority always wins, and all votes 
   are equal giving more consistent latency behavior over time.
 @[https://kafkaesque.io/performance-comparison-between-apache-pulsar-and-kafka-latency/]
   (Kafka quorum is also a WiP in KIP-250)

10.Tiered storage.
   What if you like to store messages forever (event-sourcing)?
   It can get expensive on main high-performance SSDs.
 BºWith Pulsar tiered storage you can automatically push old messages   º
 Bºinto practically infinite, cheap cloud storage (S3) and retrieve themº
 Bºjust like you do those newer, fresh-as-a-daisy messages.º
   (Kafka describes this feature in KIP-405).

11.End-to-end encryption
   Producer Java client can encrypt message using shared keys with
   the consumer. (hidden info to broker).
   Kafka is in feature-request state (KIP-317).

12.When an (stateless) Pulsar broker gets overloaded, pulsar rebalance
   request from clients automatically.
   It monitors the broker ussage of CPU, memory, and network (vs disk, since 
   brokers is stateless) to take the decision to balance.
   No need to add a new broker until all brokers are at full.
   In Kafka load-balancing is done by  installing another package 
   such as LinkedIn's Cruise Control or paying for Confluent's rebalancer
   tool.

— See also: [TODO]
@[https://streamnative.io/blog/tech/pulsar-vs-kafka-part-1]
</pre>

<pre zoom labels="kafka.devops,_doc_has.comparative,_PM.TODO">
<span title>KubeMQ: Kafka alternative</span>
@[https://dzone.com/articles/seamless-migration-from-kafka-to-kubemq?edition=699391]
• Non free product.
  · Free for stand alone single pod.
  · Professional for SMBs: Deploy on up to 20 k8s clusters.
  · Professional for Dist. product: Corporate license .
  · Enterprise: Enhanced SLA spport.
• Kafka monolithic architecture is more suited for on-premise clusters or high-end multi-VM setups. Spinning up a multi-node cluster on a standalone workstation for testing purposes can be a challenge. 
• KubeMQ messaging service is built from the ground up with Kubernetes in mind:
  - It is Statless  and Ephemeral.
  - When config. changes are needed, nodes are shut down and replaced requiring 
  - Container images is "just" 30MB, suitable for local development.
   ºzeroº-configuration setup or post-install tweakingº:
    A 'channel' is the only object developers need to create (forget about 
    brokers, exchanges, orchestrators ... KubeMQ's Raft replaces ZooKeeper).
• KubeMQ message delivery patterns include:
  • Pub/Sub with or without persistence
  • Request/Reply (synchronous, asynchronous)
  • At Most Once Delivery
  • At Least Once Delivery
  • Streaming patterns
  • RPC
  (Comparatively, Kafka only supports Pub/Sub with persistence and streaming. 
   RPC and Request/Reply patterns are not supported by Kafka at all).

• kubemqctl: cli interface analogous to kubectl.
• KubeMQ Sources allows to connect to an existing Kafka input topic to simplify 
  migration from Kafka.
</pre>




<pre zoom labels="kafka.akka,_PM.TODO">
<span xsmall>Akka+Kotlin:Data Pipes</span>
@[https://www.kotlindevelopment.com/data-pipelines-kotlin-akka-kafka/]
- Objective: 
  Create a "GitHub monitor" that build an analytics component that
  polls one of your services (GitHub), writes data into a message queue
  for later analysis, then, after post-processing, updates statistics in
  a SQL database.

- Tooling: Akka Streams + Alpakka connector collection

- "Architecture":
  1) polls GitHub Event API for kotlin activity
  2) writes all events into a Kafka topic for later use
  3) reads events from Kafka and filters out PushEvents
  4) updates a Postgres database with:
     - who pushed changes
     - when
     - into which repository.

- Akka summary: data is moving from Sources to Sinks.
  (Observable and Sink in RxJava)

ºENTRY POINTº
(standard main function)
  fun main(vararg args: String) {
    val system = ActorSystem.create()                          // "boilerplate" for using Akka and Akka Streams
    val materializer = ActorMaterializer.create(system)
    val gitHubClient = GitHubClient(system, materializer)      // instance used to poll the GitHub events API
    val eventsProducer = EventsProducer(system, materializer)  // instance used to write events into Kafka
    val eventsConsumer = EventsConsumer(system)                // instance used to read events from Kafka
    val pushEventProcessor = PushEventProcessor(materializer)  // instance used to filter PushEvents and update the database
                                                               
    eventsProducer.write(gitHubClient.events())                // put things in motion.
    pushEventProcessor.run(eventsConsumer.read())
  }

  Each time we receive a response from GitHub, we parse it and send individual events downstream.
  fun events(): Source˂JsonNode, NotUsed˃ =
    poll().flatMapConcat { response -˃
      response.nodesOpt
        .map { nodes -˃ Source.from(nodes) }
        .orElse(Source.empty())
    }

ºEventsProducer and EventsConsumerº
(the "power" of Akka Streams and Alpakka)

- Akka-Streams-Kafka greatly reduces the amount of code
  that we have to write for integrating with Kafka.
  Publishing events into a Kafka topic look like:

  fun write(events: Source˂JsonNode, NotUsed˃)
      : CompletionStage˂Done˃
        =  events.map {                                        // ← maps GitHub event to(Kafka)ProducerRecord 
             node -˃ ProducerRecord˂ByteArray, String˃
                        ("kotlin-events",
                         objectMapper.writeValueAsString(node) // ← serialize JsonNode as a String
                        ) 
      }.runWith(                        // ← connects Source Sink  
          Producer.plainSink(settings), materializer)                                         
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        - defined@Akka Streams Kafka
        - takes care of communicating with Kafka

- The other way around, reading from Kafka, is also super simple.

  fun read(): Source˂JsonNode, NotUsed˃ =
    Consumer.plainSource(
        settings,
        Subscriptions.assignmentWithOffset(
            TopicPartition("kotlin-events", 0), 0L))
       .map { record -˃ objectMapper.readTree(record.value()) }
       .mapMaterializedValue { NotUsed.getInstance() }

  At this point, we have a copy of GitHub's events feed for github.com/Kotlin stored in Kafka,
so we can time travel and run different analytics jobs on our local dataset.

ºPushEventProcessorº
 we want to filter out PushEvents from the stream
 and update a Postgres database with the results.
 - Alpakka Slick (JDBC) Connector is used to connect to PostgreSQL.
   fun createTableIfNotExists(): Source˂Int, NotUsed˃ {
     val ddl =
       """
         |CREATE TABLE IF NOT EXISTS kotlin_push_events(
         |  id         BIGINT    NOT NULL,
         |  name       VARCHAR   NOT NULL,
         |  timestamp  TIMESTAMP NOT NULL,
         |  repository VARCHAR   NOT NULL,
         |  branch     VARCHAR   NOT NULL,
         |  commits    INTEGER   NOT NULL
         |);
         |CREATE UNIQUE INDEX IF NOT EXISTS id_index ON kotlin_push_events (id);
       """.trimMargin()
     return Slick.source(session, ddl, { _ -˃ 0 })
   }

 - Similarly, the function to update the database looks like this.

   fun Source˂PushEvent, NotUsed˃.updateDatabase() :
       CompletionStage˂Done˃ =
           createTableIfNotExists().flatMapConcat { this }
       .runWith(Slick.sink˂PushEvent˃(session, 20, { event -˃
         """
           |INSERT INTO kotlin_push_events
           |(id, name, timestamp, repository, branch, commits)
           |VALUES (
           |  ${event.id},
           |  '${event.actor.login}',
           |  '${Timestamp.valueOf(event.created_at)}',
           |  '${event.repo.name}',
           |  '${event.payload.ref}',
           |  ${event.payload.distinct_size}
           |)
           |ON CONFLICT DO NOTHING
         """.trimMargin()
       }), materializer)
 
   We are almost done, what's left is filtering and mapping from JsonNode to
   PushEvent and composing the methods together.

   fun Source˂JsonNode, NotUsed˃.filterPushEvents(): Source˂PushEvent, NotUsed˃ =
     filter { node -˃ node["type"].asText() == "PushEvent" }
       .map { node -˃ objectMapper.convertValue(node, PushEvent::class.java) }

   And finally, all the functions composed together look like this.
   fun run(events: Source˂JsonNode, NotUsed˃): CompletionStage˂Done˃ =
     events
       .filterPushEvents()
       .updateDatabase()

  This is why we've used the extension methods above, so we can describe 
  transformations like this, simply chained together. 
  That's it, after running the app for a while (gradle app:run) we can see 
  the activities around different Kotlin repositories. 


You can find the complete source code on GitHub.

    A very nice property of using Akka Streams and Alpakka is that it makes 
  really easy to migrate/reuse your code, e.g. in case you want to store data 
  in Cassandra later on instead of Postgres. All you would have to do is define 
  a different Sink with CassandraSink.create. Or if GitHub events would be 
  dumped in a file located in AWS S3 instead of published to Kafka, all you 
  would have to do is create a Source with S3Client.download(bucket, key). The 
  current list of available connectors is located here, and the list is growing.
</pre>


[[}]]

<br/>

[[{]]
<span title>Unordered</span><br/>
<pre zoom labels="kafka.security,qa,distributed.ha,kafka.troubleshooting,security.backups,_PM.TODO">
<span xsmall>Unordered</span>
• Security:
  https://kafka.apache.org/documentation/#security
• Burrow Monit:
@[https://dzone.com/articles/kafka-monitoring-with-burrow]
• Best Pracites:
@[https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment]

• Mirror Maker (geo-replica):
@[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330]
  fka's mirroring feature makes it possible to maintain a replica of an 
  existing Kafka cluster. The following diagram shows how to use the 
  MirrorMaker tool to mirror a source Kafka cluster into a target 
  (mirror) Kafka cluster. The tool uses a Kafka consumer to consume 
  messages from the source cluster, and re-publishes those messages to 
  the local (target) cluster using an embedded Kafka producer.

• Kafka Backup:
@[https://github.com/itadventurer/kafka-backup]
- Kafka Backup is a tool to back up and restore your Kafka data 
  including all (configurable) topic data and especially also consumer 
  group offsets. To the best of our knowledge, Kafka Backup is the only 
  viable solution to take a cold backup of your Kafka data and restore 
  it correctly.

• Leasson Learned:
@[https://www.confluent.io/blog/stories-front-lessons-learned-supporting-apache-kafka/]
  - Under-replicated Partitions Continue to Grow Inexplicably
  - Kafka Liveness Check and Automation Causes Full Cluster Down
  - Adding a New Broker Results in Terrible Performance

• Faust (Python Streams):
@[https://github.com/robinhood/faust]
  - Faust provides both stream processing and event processing, sharing 
    similarity with tools such as Kafka Streams, Apache 
    Spark/Storm/Samza/Flink,
</pre>

- https://docs.confluent.io/current/schema-registry/index.html
  Confluent Schema Registry provides a serving layer for your metadata. It
  provides a RESTful interface for storing and retrieving Apache Avro® schemas.
  It stores a versioned history of all schemas based on a specified subject name
  strategy, provides multiple compatibility settings and allows evolution of
  schemas according to the configured compatibility settings and expanded Avro
  support. It provides serializers that plug into Apache Kafka® clients that
  handle schema storage and retrieval for Kafka messages that are sent in the
  Avro format.



