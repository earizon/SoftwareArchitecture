STORAGE ARCHITECTURE [[{storage]]
# SAN vs ...  [[{101,storage.hardware,01_PM.TODO]]
@[https://serversuit.com/community/technical-tips/view/storage-area-network,-and-other-storage-methods.html]
Storage Area Network, and Other Storage Methods
[[}]]

# Data Lake:[[{101,storage,01_PM.TODO]]
@[https://en.wikipedia.org/wiki/Data_lake]
[[}]]

# UStore!!! [[{storage.distributed,01_PM.TODO]]
@[https://arxiv.org/pdf/1702.02799.pdf]
Distributed Storage with rich semantics!!!

Today’s storage systems expose abstractions which are either  too  low-level
(e.g.,  key-value  store,  raw-blockstore)   that   they   require   developers
  to   re-invent   thewheels, or too high-level (e.g., relational databases,
Git)that they lack generality to support many classes of ap-plications.   In
this  work,  we  propose  and  implement  ageneral  distributed  data  storage
system,  called  UStore,which  has  rich  semantics.    UStore  delivers  three
 keyproperties,  namely  immutability,  sharing  and  security,which unify and
add values to many classes of today’sapplications, and which also open the
door for new ap-plications.   By  keeping  the  core  properties  within
thestorage, UStore helps reduce application development ef-forts while offering
high performance at hand. The stor-age embraces current hardware trends as key
enablers.It is built around a data-structure similar to that of Git,a  popular
source  code  versioning  system,  but  it  alsosynthesizes many designs from
distributed systems anddatabases.   Our  current  implementation  of  UStore
hasbetter  performance  than  general  in-memory  key-valuestorage systems,
especially for version scan operations.We  port  and  evaluate  four
applications  on  top  of  US-tore:  a Git-like application, a collaborative
data scienceapplication,  a transaction management application,  anda
blockchain application.   We demonstrate that UStoreenables  faster
development  and  the  UStore-backed  ap-plications can have better performance
than the existingimplementations

""" Our current implementation of UStore hasbetter performance than
general in-memory key-valuestorage systems, especially for version
scan operations.We port and evaluate four applications on top of
US-tore: a Git-like application, a collaborative data
scienceapplication, a transaction management application, anda
blockchain application. We demonstrate that UStore enables faster
development and the UStore-backed applications can have better
performance than the existing implementations. """
[[}]]

# NFS considered harmful  [[{storage.distributed,01_PM.TODO]]
@[http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html]
[[}]]

# cluster-FS comparative  [[{storage.distributed,02_doc_has.comparative,01_PM.TODO]]
@[http://zgp.org/linux-tists/20040101205016.E5998@shaitan.lightconsulting.com.html]
[[}]]


# Ceph (exabyte Soft.Defined Storage): [[{storage.ceph,distributed,01_PM.TODO]]
@[http://ceph.com/ceph-storage/]
Ceph’s RADOS provides you with extraordinary data storage  scalability—
thousands of client hosts or KVMs accessing petabytes to
exabytes of data. Each one of your applications can use the object, block or
file system interfaces to the same RADOS cluster simultaneously, which means
your Ceph storage system serves as a  flexible foundation for all of your
data storage needs. You can use Ceph for free, and deploy it on economical
commodity hardware. Ceph is a  better way to store data.

By decoupling the namespace from the underlying hardware, object-based
storage systems enable you to build much larger storage clusters. You
can scale out object-based storage systems using economical commodity hardware
, and you can replace hardware easily when it malfunctions or fails.

Ceph’s CRUSH algorithm liberates storage clusters from the scalability and
performance limitations imposed by centralized data table mapping. It
replicates and re-balance data within the cluster dynamically—elminating this
tedious task for administrators, while delivering high-performance and
infinite scalability.

See more at: http://ceph.com/ceph-storage/#sthash.KNp2tGf5.dpuf


# When Ceph is Not Enough [[{01_PM.risks]]
Josh Goldenhar, VP Product Marketing, Lightbits Labs
...  In this 30-minute webinar, we'll discuss the origins of Ceph and why
  it's a great solution for highly scalable, capacity optimized storage
  pools. You’ll learn how and where Ceph shines but also where its
  architectural shortcomings make Ceph a sub-optimal choice for today's
  high performance, scale-out databases and other key web-scale
  software infrastructure solutions.
  Participants will learn:
  • The evolution of Ceph
  • Ceph applicability to infrastructures such as OpenStack,
    OpenShift and other Kubernetes orchestration environments
  • Why Ceph can't meet the block storage challenges of modern,
    scale-out, distributed databases, analytics and AI/ML workloads
  • Where Cephs falls short on consistent latency response
  • Overcoming Ceph’s performance issues during rebuilds
  • How you can deploy high performance, low latency block storage in
    the same environments Ceph integrates with, alongside your Ceph
    deployment.  [[}]]
[[}]]

# GlusterFS [[{storage.distributed,01_PM.TODO]]
- Software defined distributed storage,
[[}]]


# Tachyon memory-centric distributed FS [[{storage.distributed,scalability,01_PM.TODO]]
@[http://tachyon-project.org/index.html]
- memory-centric distributed file system enabling reliable file
sharing at memory-speed across cluster frameworks, such as Spark and
MapReduce. It achieves high performance by leveraging lineage
information and using memory aggressively. Tachyon caches working set
files in memory, thereby avoiding going to disk to load datasets that
are frequently read. This enables different jobs/queries and
frameworks to access cached files at memory speed.

Tachyon is Hadoop compatible. Existing Spark and MapReduce programs
can run on top of it without any code change. The project is open
source (Apache License 2.0) and is deployed at multiple companies.
It has more than 40 contributors from over 15 institutions, including
Yahoo, Intel, and Redhat.

The project is the storage layer of the Berkeley Data Analytics Stack
(BDAS) and also part of the Fedora distribution.
[[}]]


# S3QL FUSE FS (Amazon S2, GCS, OpenStack,...)[[{storage.cloud,01_PM.TODO]]
- FUSE-based file system
- backed by several cloud storages:
  - such as Amazon S2, Google Cloud Storage, Rackspace CloudFiles, or OpenStack
@[http://xmodulo.com/2014/09/create-cloud-based-encrypted-file-system-linux.html]
- S3QL is one of the most popular open-source cloud-based file systems.
- full featured file system:
  - unlimited capacity
  - up to 2TB file sizes
  - compression
  - UNIX attributes
  - encryption
  - snapshots with copy-on-write
  - immutable trees
  - de-duplication
  - hardlink/symlink support, etc.
- Any bytes written to an S3QL file system are compressed/encrypted
  locally before being transmitted to cloud backend.
- When you attempt to read contents stored in an S3QL file system, the
  corresponding objects are downloaded from cloud (if not in the local
  cache), and decrypted/uncompressed on the fly.
[[}]]

# Nexenta [[{storage,01_PM.TODO]]
@[https://nexenta.com/]
- Software-Defined Storage Product Family.
[[}]]

# Minio.io [[{storage.distributed,storage.cloud,01_PM.TODO]]
@[https://minio.io]
- Private Cloud Storage
- high performance distributed object storage server, designed for
  large-scale private cloud infrastructure. Minio is widely deployed across the
  world with over 146.6M+ docker pulls.
[[}]]

# Why Object storage?  [[{storage,01_PM.TODO]]
@[https://www.ibm.com/developerworks/library/l-nilfs-exofs/index.html]

Object storage is an interesting idea and makes for a much more scalable
system. It removes portions of the file system from the host and pushes them
into the storage subsystem. There are trade-offs here, but by distributing
portions of the file system to multiple endpoints, you distribute the workload,
making the object-based method simpler to scale to much larger storage systems.
Rather than the host operating system needing to worry about block-to-file
mapping, the storage device itself provides this mapping, allowing the host to
operate at the file level.

Object storage systems also provide the ability to query the available
metadata. This provides some additional advantages, because the search
capability can be distributed to the endpoint object systems.

Object storage has made a comeback recently in the area of cloud storage. Cloud
storage providers (which sell storage as a service) represent their storage as
objects instead of the traditional block API. These providers implement APIs
for object transfer, management, and metadata management.
[[}]]

[[storage}]]
